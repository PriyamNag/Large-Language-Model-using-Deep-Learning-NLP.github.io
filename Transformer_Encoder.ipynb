{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PriyamNag/Large-Language-Model-using-Deep-Learning-NLP.github.io/blob/main/Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Upq8e_JN5xjw"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding"
      ],
      "metadata": {
        "id": "C3w2dzuSK_LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: size of vocabulary\n",
        "            embed_dim: dimension of embeddings\n",
        "        \"\"\"\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            out: embedding vector\n",
        "        \"\"\"\n",
        "        out = self.embed(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "p0gP6WMvBpyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = torch.tensor(12)\n",
        "emb = Embedding(100,512)\n",
        "print(emb(word).shape)"
      ],
      "metadata": {
        "id": "1Hc0Ybo_LUMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47478ce-832c-477f-8407-92514154f9ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "Nq3anzm2MKgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,max_seq_len,embed_model_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            seq_len: length of input sequence\n",
        "            embed_model_dim: demension of embedding\n",
        "        \"\"\"\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embed_dim = embed_model_dim\n",
        "\n",
        "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0,self.embed_dim,2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            x: output\n",
        "        \"\"\"\n",
        "\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.embed_dim)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GyEv4WVvL5xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multihead Attention"
      ],
      "metadata": {
        "id": "9L5It-2QQn_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim=512, n_heads=8):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim: dimension of embeding vector output\n",
        "            n_heads: number of self attention heads\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.single_head_dim = int(self.embed_dim / self.n_heads)\n",
        "\n",
        "\n",
        "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)\n",
        "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
        "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
        "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim)\n",
        "\n",
        "    def forward(self,key,query,value,mask=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key : key vector\n",
        "           query : query vector\n",
        "           value : value vector\n",
        "           mask: mask for decoder\n",
        "\n",
        "        Returns:\n",
        "           output vector from multihead attention\n",
        "        \"\"\"\n",
        "        batch_size = key.size(0)\n",
        "        seq_length = key.size(1)\n",
        "\n",
        "        seq_length_query = query.size(1)\n",
        "\n",
        "\n",
        "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
        "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim)\n",
        "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim)\n",
        "\n",
        "        k = self.key_matrix(key)\n",
        "        q = self.query_matrix(query)\n",
        "        v = self.value_matrix(value)\n",
        "\n",
        "        q = q.transpose(1,2)\n",
        "        k = k.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # computes attention\n",
        "        # adjust key for matrix multiplication\n",
        "        k_adjusted = k.transpose(-1,-2)\n",
        "        product = torch.matmul(q, k_adjusted)\n",
        "\n",
        "\n",
        "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
        "        if mask is not None:\n",
        "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        #divising by square root of key dimension\n",
        "        product = product / math.sqrt(self.single_head_dim)\n",
        "\n",
        "        #applying softmax\n",
        "        scores = F.softmax(product, dim=-1)\n",
        "\n",
        "        #mutiply with value matrix\n",
        "        scores = torch.matmul(scores, v)\n",
        "\n",
        "        #concatenated output\n",
        "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)\n",
        "\n",
        "        output = self.out(concat)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "z3dG_fjoMzCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Encoder Block"
      ],
      "metadata": {
        "id": "UEBUNj_DRAVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           embed_dim: dimension of the embedding\n",
        "           expansion_factor: fator ehich determines output dimension of linear layer\n",
        "           n_heads: number of attention heads\n",
        "\n",
        "        \"\"\"\n",
        "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.feed_forward = nn.Sequential(\n",
        "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self,key,query,value):\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           key: key vector\n",
        "           query: query vector\n",
        "           value: value vector\n",
        "           norm2_out: output of transformer block\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        attention_out = self.attention(key,query,value)\n",
        "        attention_residual_out = attention_out + value\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual_out))\n",
        "\n",
        "        feed_fwd_out = self.feed_forward(norm1_out)\n",
        "        feed_fwd_residual_out = feed_fwd_out + norm1_out\n",
        "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n",
        "\n",
        "        return norm2_out\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        seq_len : length of input sequence\n",
        "        embed_dim: dimension of embedding\n",
        "        num_layers: number of encoder layers\n",
        "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
        "        n_heads: number of heads in multihead attention\n",
        "\n",
        "    Returns:\n",
        "        out: output of the encoder\n",
        "    \"\"\"\n",
        "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed_out = self.embedding_layer(x)\n",
        "        out = self.positional_encoder(embed_out)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out,out,out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "AGlIv_ScPA0_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}